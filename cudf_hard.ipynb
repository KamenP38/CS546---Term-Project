{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdfbf81-aaef-4194-ad89-3af152e23e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",base_hard_pandas_queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_mempolicy: Function not implemented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File results_cudf_hard_queries_csv/results_cudf_csv_pandas_plain_hard_25m_TEST.txt created/updated successfully!\n",
      "hard,Q1,558.418375492096,200024560\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import numba\n",
    "import cython\n",
    "from time import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import json\n",
    "import resource\n",
    "import numpy as np\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import rmm\n",
    "import itertools\n",
    "from dask import delayed\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dask.base import normalize_token\n",
    "\n",
    "# Register custom tokenization for cudf.Index\n",
    "@normalize_token.register(cudf.core.index.Index)\n",
    "def normalize_cudf_index(index):\n",
    "    return normalize_token(type(index)), tuple(index.to_pandas().values)\n",
    "\n",
    "# Custom tokenization for cudf.MultiIndex\n",
    "@normalize_token.register(cudf.MultiIndex)\n",
    "def normalize_cudf_multiindex(index):\n",
    "    return normalize_token(type(index)), tuple(index.to_pandas().values)\n",
    "\n",
    "indices = ['filename', 'application', 'io_zone', 'redundancy_type']\n",
    "combinations = list(it.combinations(indices, r=2))[:5]\n",
    "\n",
    "# Log results to a file\n",
    "def log_results_to_file(result_file, time, memory):\n",
    "    with open(result_file, 'a') as f:\n",
    "        f.write(f\"Times: {time}, Memory: {memory}\\n\")\n",
    "    \n",
    "    if os.path.exists(result_file):\n",
    "        print(f\"File {result_file} created/updated successfully!\")\n",
    "    else:\n",
    "        print(f\"Error: {result_file} was not created.\")\n",
    "\n",
    "def base_hard_cudf_queries(log_dir, result_file):\n",
    "    print(f\",base_hard_pandas_queries\")\n",
    "    df = cudf.read_csv(log_dir)\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1\n",
    "            t1 = time()\n",
    "            res = df.groupby([ix]).agg({iy: list, col: 'sum'}).explode(iy).reset_index().groupby([iy]).agg({ix: list, col: 'sum'})\n",
    "            time_elapsed = time() - t1\n",
    "            memory_usage = res.memory_usage(deep=True).sum()\n",
    "\n",
    "            # Store the time and memory usage for graphing later\n",
    "            log_results_to_file(result_file, time_elapsed, memory_usage)\n",
    "\n",
    "            print(f\"hard,Q{query_index},{time_elapsed},{memory_usage}\")\n",
    "\n",
    "def base_hard_dask_queries(log_dir, result_file):\n",
    "    print(f\",base_hard_dask_queries\")\n",
    "    \n",
    "    # Read the data into a Dask cuDF DataFrame\n",
    "    ddf = dask_cudf.read_csv(log_dir)\n",
    "    \n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1\n",
    "            t1 = time()\n",
    "\n",
    "            res = ddf.groupby([ix]).agg({iy: \"collect\", col: 'sum'}).reset_index()\n",
    "            \n",
    "            meta = {\n",
    "                ix: \"object\",           # generic types to avoid conflicts\n",
    "                iy: \"object\",           # generic types to avoid conflicts\n",
    "                col: \"int64\"\n",
    "            }\n",
    "\n",
    "            res_exploded = res.map_partitions(lambda df: df.explode(iy), meta=meta)\n",
    "\n",
    "            final_res = res_exploded.groupby(iy).agg({ix: \"collect\", col: 'sum'}).reset_index().compute()\n",
    "\n",
    "            time_elapsed = time() - t1\n",
    "            memory_usage = final_res.memory_usage(deep=True).sum()\n",
    "\n",
    "            log_results_to_file(result_file, time_elapsed, memory_usage)\n",
    "            print(f\"hard,Q{query_index}, {time_elapsed}, Memory: {memory_usage}\")\n",
    "\n",
    "\n",
    "def iomax_hard_cudf_queries(log_dir, result_file):\n",
    "    print(f\",iomax_hard_pandas_queries\")\n",
    "    df = cudf.read_csv(log_dir)\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1 \n",
    "            t1 = time()\n",
    "            m = 0\n",
    "            if query_index == 1:\n",
    "                x = df.groupby(indices).agg({'request_io_size_bytes': 'sum', 'disk_time': 'sum'}).reset_index()\n",
    "                m = x.memory_usage(deep=True).sum()\n",
    "            x.groupby([ix]) \\\n",
    "                .agg({iy: list, col: 'sum'}) \\\n",
    "                .reset_index() \\\n",
    "                .explode(iy) \\\n",
    "                .groupby([iy]) \\\n",
    "                .agg({ix: list, col: 'sum'})\n",
    "            time_elapsed = time() - t1\n",
    "\n",
    "            # Store the time and memory usage for graphing later\n",
    "            log_results_to_file(result_file, time_elapsed, m)\n",
    "\n",
    "            print(f\"{time_elapsed},{m}\")\n",
    "\n",
    "def iomax_hard_dask_queries(log_dir, result_file):\n",
    "    print(f\",iomax_hard_dask_queries\")\n",
    "    \n",
    "    # Load data into a Dask-cuDF DataFrame\n",
    "    ddf = dask_cudf.read_csv(log_dir)\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1\n",
    "            t1 = time()            \n",
    "            m = 0\n",
    "            if query_index == 1:\n",
    "                x = ddf.groupby(indices).agg({'request_io_size_bytes': 'sum', 'disk_time': 'sum'}).reset_index().compute()\n",
    "                m = x.memory_usage(deep=True).sum()   \n",
    "            \n",
    "            x.groupby([ix]).agg({iy: list, col: 'sum'}).reset_index().explode(iy).groupby([iy]).agg({ix: list, col: 'sum'})\n",
    "            \n",
    "            time_elapsed = time() - t1\n",
    "\n",
    "            log_results_to_file(result_file, time_elapsed, m)\n",
    "            print(f\"hard,Q{query_index}, {time_elapsed}, Memory: {m}\")\n",
    "            \n",
    "base_hard_pandas_queries('datasets_thesios_io_traces/dataset-25m.csv', 'results_cudf_hard_queries_csv/results_cudf_csv_pandas_plain_hard_25m_TEST.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48da059b-d9be-4c8c-814d-d8259cef303a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
