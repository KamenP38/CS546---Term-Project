{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f646890c-b51e-4ec8-b860-43ca24386fdd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# The params below are needed if the researcher wants to run the notebook in isolation with papermill\n",
    "# query_function = None\n",
    "# dataset_path = None\n",
    "# result_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7501f766-99ef-4b06-98f9-0d8948c0d600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",iomax_medium_pandas_queries\n",
      "File results_medium_queries_csv/results_pandas_iomax_medium_25m.txt created/updated successfully!\n",
      "medium,Q1,8.7985360622406,131945601\n",
      "File results_medium_queries_csv/results_pandas_iomax_medium_25m.txt created/updated successfully!\n",
      "medium,Q2,1.7559683322906494,0\n",
      "File results_medium_queries_csv/results_pandas_iomax_medium_25m.txt created/updated successfully!\n",
      "medium,Q3,4.037119626998901,0\n",
      "File results_medium_queries_csv/results_pandas_iomax_medium_25m.txt created/updated successfully!\n",
      "medium,Q4,1.6657686233520508,0\n",
      "File results_medium_queries_csv/results_pandas_iomax_medium_25m.txt created/updated successfully!\n",
      "medium,Q5,1.7075364589691162,0\n",
      "File results_medium_queries_csv/results_pandas_iomax_medium_25m.txt created/updated successfully!\n",
      "medium,Q6,1.6552934646606445,0\n",
      "File results_medium_queries_csv/results_pandas_iomax_medium_25m.txt created/updated successfully!\n",
      "medium,Q7,0.07173347473144531,0\n",
      "File results_medium_queries_csv/results_pandas_iomax_medium_25m.txt created/updated successfully!\n",
      "medium,Q8,0.07300972938537598,0\n",
      "File results_medium_queries_csv/results_pandas_iomax_medium_25m.txt created/updated successfully!\n",
      "medium,Q9,0.07795405387878418,0\n",
      "File results_medium_queries_csv/results_pandas_iomax_medium_25m.txt created/updated successfully!\n",
      "medium,Q10,0.07832837104797363,0\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow\n",
    "import numba\n",
    "import cython\n",
    "from time import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import numpy as np\n",
    "import json\n",
    "import resource\n",
    "import numpy as np\n",
    "\n",
    "indices = ['filename', 'application', 'io_zone', 'redundancy_type']\n",
    "combinations = list(it.combinations(indices, r=2))[:5]\n",
    "\n",
    "# Function to log results to a file\n",
    "def log_results_to_file(result_file, time, memory):\n",
    "    with open(result_file, 'a') as f:\n",
    "        f.write(f\"Times: {time}, Memory: {memory}\\n\")\n",
    "    \n",
    "    # Check if the file was created\n",
    "    if os.path.exists(result_file):\n",
    "        print(f\"File {result_file} created/updated successfully!\")\n",
    "    else:\n",
    "        print(f\"Error: {result_file} was not created.\")\n",
    "\n",
    "def base_medium_pandas_queries(log_dir, result_file):\n",
    "    print(f\",base_medium_pandas_queries\")\n",
    "    df = pd.read_csv(log_dir)\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1 \n",
    "            t1 = time()\n",
    "            res = df.groupby([ix, iy]).agg({col: 'sum'}).groupby([ix]).sum()\n",
    "            time_elapsed = time() - t1\n",
    "            memory_usage = res.memory_usage(deep=True).sum()\n",
    "\n",
    "            # Store the time and memory usage for graphing later\n",
    "            log_results_to_file(result_file, time_elapsed, memory_usage)\n",
    "\n",
    "            print(f\"medium,Q{query_index},{time_elapsed},{memory_usage}\")\n",
    "            \n",
    "\n",
    "def base_medium_dask_queries(log_dir, result_file):\n",
    "    print(f\",base_medium_dask_queries\")\n",
    "    ddf = dd.read_csv(log_dir)\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1 \n",
    "            t1 = time()\n",
    "            res = ddf.groupby([ix, iy]).agg({col: 'sum'}).groupby([ix]).sum().compute()\n",
    "            time_elapsed = time() - t1\n",
    "            memory_usage = res.memory_usage(deep=True).sum()\n",
    "\n",
    "            # Store the time and memory usage for graphing later\n",
    "            log_results_to_file(result_file, time_elapsed, memory_usage)\n",
    "\n",
    "            print(f\"medium,Q{query_index},{time_elapsed},{memory_usage}\")\n",
    "\n",
    "def iomax_medium_pandas_queries(log_dir, result_file):\n",
    "    print(f\",iomax_medium_pandas_queries\")\n",
    "    df = pd.read_csv(log_dir)\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1 \n",
    "            t1 = time()\n",
    "            m = 0\n",
    "            if query_index == 1:\n",
    "                x = df.groupby(indices).agg({'request_io_size_bytes': 'sum', 'disk_time': 'sum'})\n",
    "                m = x.memory_usage(deep=True).sum()\n",
    "            x.groupby([ix, iy]).agg({col: 'sum'}).groupby([ix]).sum()\n",
    "            time_elapsed = time() - t1\n",
    "\n",
    "            # Store the time and memory usage for graphing later\n",
    "            log_results_to_file(result_file, time_elapsed, m)\n",
    "\n",
    "            print(f\"medium,Q{query_index},{time_elapsed},{m}\")\n",
    "\n",
    "def iomax_medium_dask_queries(log_dir, result_file):\n",
    "    print(f\",iomax_medium_dask_queries\")\n",
    "    ddf = dd.read_csv(log_dir)\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1 \n",
    "            t1 = time()\n",
    "            m = 0\n",
    "            if query_index == 1:\n",
    "                x = ddf.groupby(indices).agg({'request_io_size_bytes': 'sum', 'disk_time': 'sum'}).compute()\n",
    "                m = x.memory_usage(deep=True).sum()\n",
    "            x.groupby([ix, iy]).agg({col: 'sum'}).groupby([ix]).sum()\n",
    "            time_elapsed = time() - t1\n",
    "\n",
    "            # Store the time and memory usage for graphing later\n",
    "            log_results_to_file(result_file, time_elapsed, m)\n",
    "\n",
    "            print(f\"medium,Q{query_index},{time_elapsed},{m}\")\n",
    "\n",
    "iomax_medium_pandas_queries(\"datasets_thesios_io_traces/dataset-25m.csv\", \"results_medium_queries_csv/results_pandas_iomax_medium_25m.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ca1731-b1ab-48fe-b21c-7460034aba7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
