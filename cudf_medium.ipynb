{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9277def2-c036-4af3-9739-f95d7c2c1c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",iomax_medium_dask_queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "get_mempolicy: Function not implemented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt created/updated successfully!\n",
      "medium,Q1,224.6024146080017,1032070996\n",
      "File results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt created/updated successfully!\n",
      "medium,Q2,0.47611522674560547,0\n",
      "File results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt created/updated successfully!\n",
      "medium,Q3,0.4142496585845947,0\n",
      "File results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt created/updated successfully!\n",
      "medium,Q4,0.41880297660827637,0\n",
      "File results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt created/updated successfully!\n",
      "medium,Q5,0.36164069175720215,0\n",
      "File results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt created/updated successfully!\n",
      "medium,Q6,0.37160444259643555,0\n",
      "File results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt created/updated successfully!\n",
      "medium,Q7,0.12158823013305664,0\n",
      "File results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt created/updated successfully!\n",
      "medium,Q8,0.13793659210205078,0\n",
      "File results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt created/updated successfully!\n",
      "medium,Q9,0.12509894371032715,0\n",
      "File results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt created/updated successfully!\n",
      "medium,Q10,0.12178635597229004,0\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import itertools as it\n",
    "import numpy as np\n",
    "import pyarrow\n",
    "import numba\n",
    "import cython\n",
    "from time import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import json\n",
    "import resource\n",
    "import numpy as np\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import rmm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from dask.base import normalize_token\n",
    "\n",
    "# Register custom tokenization for cudf.Index\n",
    "@normalize_token.register(cudf.core.index.Index)\n",
    "def normalize_cudf_index(index):\n",
    "    return normalize_token(type(index)), tuple(index.to_pandas().values)\n",
    "\n",
    "# Custom tokenization for cudf.MultiIndex\n",
    "@normalize_token.register(cudf.MultiIndex)\n",
    "def normalize_cudf_multiindex(index):\n",
    "    return normalize_token(type(index)), tuple(index.to_pandas().values)\n",
    "\n",
    "indices = ['filename', 'application', 'io_zone', 'redundancy_type']\n",
    "combinations = list(it.combinations(indices, r=2))[:5]\n",
    "\n",
    "# Log results to a file\n",
    "def log_results_to_file(result_file, time, memory):\n",
    "    with open(result_file, 'a') as f:\n",
    "        f.write(f\"Times: {time}, Memory: {memory}\\n\")\n",
    "    \n",
    "    if os.path.exists(result_file):\n",
    "        print(f\"File {result_file} created/updated successfully!\")\n",
    "    else:\n",
    "        print(f\"Error: {result_file} was not created.\")\n",
    "\n",
    "# Pandas query function\n",
    "def base_medium_cudf_queries(log_dir, result_file):\n",
    "    print(f\",base_medium_pandas_queries\")\n",
    "    df = cudf.read_csv(log_dir)\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1 \n",
    "            t1 = time()\n",
    "            res = df.groupby([ix, iy]).agg({col: 'sum'}).groupby([ix]).sum()\n",
    "            time_elapsed = time() - t1\n",
    "            memory_usage = res.memory_usage(deep=True).sum()\n",
    "\n",
    "            # Store the time and memory usage for graphing later\n",
    "            log_results_to_file(result_file, time_elapsed, memory_usage)\n",
    "\n",
    "            print(f\"medium,Q{query_index},{time_elapsed},{memory_usage}\")\n",
    "    \n",
    "# Dask query function\n",
    "def base_medium_dask_queries(log_dir, result_file):\n",
    "    print(f\",base_medium_dask_queries\")\n",
    "    ddf = dask_cudf.read_csv(f\"{log_dir}\")\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1 \n",
    "            t1 = time()\n",
    "            res = ddf.groupby([ix, iy]).agg({col: 'sum'}).groupby([ix]).sum().compute()\n",
    "            time_elapsed = time() - t1\n",
    "            memory_usage = res.memory_usage(deep=True).sum()\n",
    "\n",
    "            # Store the time and memory usage for graphing later\n",
    "            log_results_to_file(result_file, time_elapsed, memory_usage)\n",
    "\n",
    "            print(f\"medium,Q{query_index},{time_elapsed},{memory_usage}\")\n",
    "\n",
    "# Pandas IOMAX queries\n",
    "def iomax_medium_cudf_queries(log_dir, result_file):\n",
    "    print(f\",iomax_medium_pandas_queries\")\n",
    "    df = cudf.read_csv(log_dir)\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1 \n",
    "            t1 = time()\n",
    "            m = 0\n",
    "            if query_index == 1:\n",
    "                x = df.groupby(indices).agg({'request_io_size_bytes': 'sum', 'disk_time': 'sum'})\n",
    "                m = x.memory_usage(deep=True).sum()\n",
    "            x.groupby([ix, iy]).agg({col: 'sum'}).groupby([ix]).sum()\n",
    "            time_elapsed = time() - t1\n",
    "\n",
    "            # Store the time and memory usage for graphing later\n",
    "            log_results_to_file(result_file, time_elapsed, m)\n",
    "\n",
    "            print(f\"medium,Q{query_index},{time_elapsed},{m}\")\n",
    "    \n",
    "# Dask IOMAX queries\n",
    "def iomax_medium_dask_queries(log_dir, result_file):\n",
    "    print(f\",iomax_medium_dask_queries\")\n",
    "    ddf = dask_cudf.read_csv(f\"{log_dir}\")\n",
    "    query_index = 0\n",
    "    for ix, iy in combinations:\n",
    "        for col in ['request_io_size_bytes', 'disk_time']:\n",
    "            query_index += 1 \n",
    "            t1 = time()\n",
    "            m = 0\n",
    "            if query_index == 1:\n",
    "                x = ddf.groupby(indices).agg({'request_io_size_bytes': 'sum', 'disk_time': 'sum'}).compute()\n",
    "                m = x.memory_usage(deep=True).sum()\n",
    "            x.groupby([ix, iy]).agg({col: 'sum'}).groupby([ix]).sum()\n",
    "            time_elapsed = time() - t1\n",
    "\n",
    "            # Store the time and memory usage for graphing later\n",
    "            log_results_to_file(result_file, time_elapsed, m)\n",
    "\n",
    "            print(f\"medium,Q{query_index},{time_elapsed},{m}\")\n",
    "\n",
    "iomax_medium_dask_queries('datasets_thesios_io_traces/dataset-125m.csv', 'results_cudf_medium_queries_csv/results_cudf_csv_dask_iomax_medium_125m.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1519389f-18e1-4ace-9701-ea690a9597d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
